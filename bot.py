"""
ğŸ” OKAZJE BOT â€” Telegram bot do analizy okazji kolekcjonerskich
Wklej link â†’ dostaniesz analizÄ™ AI (oryginaÅ‚/replika, wycena, werdykt)
Auto-monitoring Sprzedajemy.pl i Gratka.pl co 30 minut
"""

import os
import re
import json
import logging
import asyncio
import hashlib
from datetime import datetime, timedelta
from dataclasses import dataclass, field, asdict
from typing import Optional

import httpx
from telegram import Update, Bot
from telegram.ext import (
    Application,
    CommandHandler,
    MessageHandler,
    filters,
    ContextTypes,
)
from bs4 import BeautifulSoup
import anthropic

# â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

TELEGRAM_TOKEN = os.environ["TELEGRAM_TOKEN"]
ANTHROPIC_API_KEY = os.environ["ANTHROPIC_API_KEY"]
CHAT_ID = os.environ.get("CHAT_ID", "")  # TwÃ³j Telegram chat ID (opcjonalnie, do auto-alertÃ³w)

SCAN_INTERVAL_MINUTES = int(os.environ.get("SCAN_INTERVAL", "30"))
MAX_PRICE = int(os.environ.get("MAX_PRICE", "550"))
MIN_MARGIN_PERCENT = int(os.environ.get("MIN_MARGIN", "200"))

# SÅ‚owa kluczowe do monitoringu â€” edytuj przez /keywords w bocie
DEFAULT_KEYWORDS = [
    "komiks PRL",
    "Relax komiks",
    "Kapitan Å»bik",
    "figurka Ä†mielÃ³w",
    "porcelana PRL",
    "zegarek BÅ‚onie",
    "zegarek Rakieta",
    "zegarek Wostok",
    "obraz olejny",
    "szabla",
    "bagnet",
    "Lem pierwsze wydanie",
    "Sapkowski wydanie",
    "ikona prawosÅ‚awna",
    "sztuÄ‡ce srebrne",
    "kordelas",
]

# â”€â”€ LOGGING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

logging.basicConfig(
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    level=logging.INFO,
)
logger = logging.getLogger("okazje-bot")

# â”€â”€ DATA MODEL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@dataclass
class Offer:
    url: str
    title: str
    price: float
    description: str
    location: str
    platform: str
    seller: str = ""
    condition: str = ""
    images: list = field(default_factory=list)
    scraped_at: str = ""
    analysis: str = ""
    verdict: str = ""  # "BUY", "NEGOTIATE", "SKIP", "INVESTIGATE"
    estimated_value_low: float = 0
    estimated_value_high: float = 0

    @property
    def id(self) -> str:
        return hashlib.md5(self.url.encode()).hexdigest()[:12]

    @property
    def margin_low(self) -> float:
        if self.price <= 0:
            return 0
        return ((self.estimated_value_low - self.price) / self.price) * 100

    @property
    def margin_high(self) -> float:
        if self.price <= 0:
            return 0
        return ((self.estimated_value_high - self.price) / self.price) * 100


# â”€â”€ SCRAPERS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept-Language": "pl-PL,pl;q=0.9,en-US;q=0.8,en;q=0.7",
}


async def scrape_url(url: str) -> Optional[Offer]:
    """Pobierz dane z dowolnego linku â€” wykrywa platformÄ™ automatycznie."""
    try:
        if "sprzedajemy.pl" in url:
            return await scrape_sprzedajemy(url)
        elif "olx.pl" in url:
            return await scrape_olx(url)
        elif "allegro.pl" in url:
            return await scrape_allegro(url)
        elif "vinted.pl" in url:
            return await scrape_vinted(url)
        elif "gratka.pl" in url:
            return await scrape_gratka(url)
        else:
            return await scrape_generic(url)
    except Exception as e:
        logger.error(f"Scraping error for {url}: {e}")
        return None


async def scrape_sprzedajemy(url: str) -> Optional[Offer]:
    """Scraper dla Sprzedajemy.pl â€” relatywnie Å‚atwa strona do parsowania."""
    async with httpx.AsyncClient(headers=HEADERS, follow_redirects=True, timeout=15) as client:
        resp = await client.get(url)
        resp.raise_for_status()

    soup = BeautifulSoup(resp.text, "html.parser")

    # TytuÅ‚
    title_tag = soup.find("h1")
    title = title_tag.get_text(strip=True) if title_tag else "Brak tytuÅ‚u"

    # Cena
    price = 0.0
    price_patterns = [
        soup.find("span", class_=re.compile(r"price|cena", re.I)),
        soup.find("strong", string=re.compile(r"\d+.*zÅ‚")),
    ]
    # Szukaj ceny w tekÅ›cie strony
    price_match = re.search(r'(\d[\d\s]*(?:[.,]\d{2})?)\s*zÅ‚', resp.text)
    if price_match:
        price_str = price_match.group(1).replace(" ", "").replace(",", ".")
        try:
            price = float(price_str)
        except ValueError:
            pass

    # Opis
    desc = ""
    # Sprzedajemy.pl ma opis po nagÅ‚Ã³wku "SzczegÃ³Å‚y ogÅ‚oszenia"
    desc_candidates = soup.find_all("div", class_=re.compile(r"desc|opis|content", re.I))
    if desc_candidates:
        desc = "\n".join(d.get_text(strip=True) for d in desc_candidates[:3])
    if not desc:
        # Fallback: zbierz tekst z body
        body_text = soup.get_text(separator="\n", strip=True)
        # WyciÄ…gnij sekcjÄ™ opisu
        for keyword in ["Polecam", "Sprzedam", "OferujÄ™", "Zapraszam", "Stan:"]:
            idx = body_text.find(keyword)
            if idx != -1:
                desc = body_text[idx : idx + 500]
                break
        if not desc:
            desc = body_text[:500]

    # Lokalizacja
    location = ""
    loc_match = re.search(r'(Bielsko-BiaÅ‚a|Katowice|KrakÃ³w|Warszawa|[\w\s-]+),\s*([\w]+kie)', resp.text)
    if loc_match:
        location = loc_match.group(0)

    # Stan
    condition = ""
    if "nowe" in resp.text.lower():
        condition = "nowe"
    elif "uÅ¼ywane" in resp.text.lower():
        condition = "uÅ¼ywane"

    # Sprzedawca
    seller = ""
    seller_match = re.search(r'class="[^"]*user[^"]*"[^>]*>([^<]+)', resp.text)

    # Obrazki
    images = [img.get("src", "") for img in soup.find_all("img") if "thumbs" in str(img.get("src", ""))]

    return Offer(
        url=url,
        title=title,
        price=price,
        description=desc[:1000],
        location=location,
        platform="sprzedajemy.pl",
        seller=seller,
        condition=condition,
        images=images[:5],
        scraped_at=datetime.now().isoformat(),
    )


async def scrape_gratka(url: str) -> Optional[Offer]:
    """Scraper dla Gratka.pl."""
    async with httpx.AsyncClient(headers=HEADERS, follow_redirects=True, timeout=15) as client:
        resp = await client.get(url)
        resp.raise_for_status()

    soup = BeautifulSoup(resp.text, "html.parser")

    title_tag = soup.find("h1")
    title = title_tag.get_text(strip=True) if title_tag else "Brak tytuÅ‚u"

    price = 0.0
    price_match = re.search(r'(\d[\d\s]*(?:[.,]\d{2})?)\s*zÅ‚', resp.text)
    if price_match:
        price_str = price_match.group(1).replace(" ", "").replace(",", ".")
        try:
            price = float(price_str)
        except ValueError:
            pass

    body_text = soup.get_text(separator="\n", strip=True)

    return Offer(
        url=url,
        title=title,
        price=price,
        description=body_text[:1000],
        location="",
        platform="gratka.pl",
        scraped_at=datetime.now().isoformat(),
    )


async def scrape_olx(url: str) -> Optional[Offer]:
    """OLX â€” wyciÄ…ga dane z JSON-LD i __NEXT_DATA__ w HTML."""
    async with httpx.AsyncClient(headers=HEADERS, follow_redirects=True, timeout=15) as client:
        resp = await client.get(url)
        resp.raise_for_status()

    soup = BeautifulSoup(resp.text, "html.parser")
    html_text = resp.text

    title = ""
    price = 0.0
    description = ""
    location = ""
    condition = ""
    seller = ""
    images = []

    # Metoda 1: __NEXT_DATA__ (OLX uÅ¼ywa Next.js)
    next_data_script = soup.find("script", id="__NEXT_DATA__")
    if next_data_script:
        try:
            next_data = json.loads(next_data_script.string)
            props = next_data.get("props", {}).get("pageProps", {})
            ad = props.get("ad", {})

            if ad:
                title = ad.get("title", "")
                desc_raw = ad.get("description", "")
                description = desc_raw

                # Cena
                price_info = ad.get("price", {})
                if isinstance(price_info, dict):
                    price_val = price_info.get("regularPrice", {}).get("value", 0)
                    if not price_val:
                        price_val = price_info.get("value", 0)
                    price = float(price_val) if price_val else 0.0

                # Lokalizacja
                loc_info = ad.get("location", {})
                if loc_info:
                    city = loc_info.get("cityName", "")
                    region = loc_info.get("regionName", "")
                    location = f"{city}, {region}" if city else region

                # Stan (z parametrÃ³w)
                params = ad.get("params", [])
                for p in params:
                    if p.get("key") == "state":
                        condition = p.get("normalizedValue", p.get("value", {}).get("label", ""))
                    # Dodaj inne parametry do opisu
                    param_name = p.get("name", "")
                    param_val = p.get("value", {})
                    if isinstance(param_val, dict):
                        param_label = param_val.get("label", "")
                    else:
                        param_label = str(param_val)
                    if param_name and param_label:
                        description += f"\n{param_name}: {param_label}"

                # Sprzedawca
                user_info = ad.get("user", {})
                seller = user_info.get("name", "")

                # ZdjÄ™cia
                photos = ad.get("photos", [])
                images = [p.get("link", "") for p in photos[:8] if p.get("link")]
                if images:
                    description += f"\nZdjÄ™cia: {len(images)} szt."

        except (json.JSONDecodeError, KeyError, TypeError) as e:
            logger.warning(f"OLX __NEXT_DATA__ parse error: {e}")

    # Metoda 2: JSON-LD fallback
    if not title:
        for script in soup.find_all("script", type="application/ld+json"):
            try:
                data = json.loads(script.string)
                if isinstance(data, dict):
                    if data.get("@type") == "Product" or "offers" in data:
                        title = data.get("name", "")
                        desc_ld = data.get("description", "")
                        if desc_ld:
                            description = desc_ld
                        offers = data.get("offers", {})
                        if isinstance(offers, dict):
                            price = float(offers.get("price", 0))
                        img = data.get("image", [])
                        if isinstance(img, list):
                            images = img[:8]
                        elif isinstance(img, str):
                            images = [img]
                        break
            except (json.JSONDecodeError, ValueError):
                pass

    # Metoda 3: HTML fallback
    if not title:
        title_tag = soup.find("h1") or soup.find("title")
        title = title_tag.get_text(strip=True) if title_tag else "Brak tytuÅ‚u"

    if price == 0:
        price_match = re.search(r'(\d[\d\s]*(?:[.,]\d{2})?)\s*zÅ‚', html_text)
        if price_match:
            try:
                price = float(price_match.group(1).replace(" ", "").replace(",", "."))
            except ValueError:
                pass

    if not description:
        description = soup.get_text(separator="\n", strip=True)[:1000]

    return Offer(
        url=url,
        title=title or "Brak tytuÅ‚u",
        price=price,
        description=description[:1500],
        location=location,
        platform="olx.pl",
        seller=seller,
        condition=condition,
        images=images,
        scraped_at=datetime.now().isoformat(),
    )


async def scrape_allegro(url: str) -> Optional[Offer]:
    """Allegro â€” wyciÄ…ga dane z JSON-LD i meta tagÃ³w."""
    async with httpx.AsyncClient(headers=HEADERS, follow_redirects=True, timeout=15) as client:
        resp = await client.get(url)
        resp.raise_for_status()

    soup = BeautifulSoup(resp.text, "html.parser")
    html_text = resp.text

    title = ""
    price = 0.0
    description = ""
    condition = ""
    images = []
    seller = ""
    location = ""

    # JSON-LD
    for script in soup.find_all("script", type="application/ld+json"):
        try:
            data = json.loads(script.string)
            if isinstance(data, dict):
                if data.get("@type") == "Product" or "offers" in data:
                    title = data.get("name", "")
                    desc_ld = data.get("description", "")
                    if desc_ld:
                        description = desc_ld
                    offers = data.get("offers", {})
                    if isinstance(offers, dict):
                        price = float(offers.get("price", 0))
                        condition = offers.get("itemCondition", "")
                        seller_info = offers.get("seller", {})
                        if isinstance(seller_info, dict):
                            seller = seller_info.get("name", "")
                    img = data.get("image", [])
                    if isinstance(img, list):
                        images = img[:8]
                    elif isinstance(img, str):
                        images = [img]
                    break
        except (json.JSONDecodeError, ValueError):
            pass

    # Meta tags fallback
    if not title:
        og_title = soup.find("meta", {"property": "og:title"})
        if og_title:
            title = og_title.get("content", "")
    if not title:
        title_tag = soup.find("h1") or soup.find("title")
        title = title_tag.get_text(strip=True) if title_tag else "Brak tytuÅ‚u"

    if price == 0:
        meta_price = soup.find("meta", {"property": "product:price:amount"})
        if meta_price:
            try:
                price = float(meta_price["content"])
            except (ValueError, KeyError):
                pass

    if price == 0:
        price_match = re.search(r'(\d[\d\s]*(?:[.,]\d{2})?)\s*zÅ‚', html_text)
        if price_match:
            try:
                price = float(price_match.group(1).replace(" ", "").replace(",", "."))
            except ValueError:
                pass

    if not description:
        # WyciÄ…gnij co siÄ™ da z body
        description = soup.get_text(separator="\n", strip=True)[:1000]

    if images:
        description += f"\nZdjÄ™cia: {len(images)} szt."

    # WyczyÅ›Ä‡ condition z URL-a schema.org
    if condition and "schema.org" in condition:
        condition = condition.split("/")[-1]  # np. "UsedCondition" â†’ "UsedCondition"

    return Offer(
        url=url,
        title=title or "Brak tytuÅ‚u",
        price=price,
        description=description[:1500],
        location=location,
        platform="allegro.pl",
        seller=seller,
        condition=condition,
        images=images,
        scraped_at=datetime.now().isoformat(),
    )


async def scrape_vinted(url: str) -> Optional[Offer]:
    """Vinted â€” uÅ¼ywa wewnÄ™trznego API do pobrania peÅ‚nych danych."""
    # WyciÄ…gnij item ID z URL
    item_match = re.search(r'/items/(\d+)', url)
    if not item_match:
        # SprÃ³buj alternatywny format
        item_match = re.search(r'(\d{8,})', url)
    if not item_match:
        return await scrape_generic(url)

    item_id = item_match.group(1)

    # Vinted wymaga sesji â€” najpierw pobierz cookie
    vinted_headers = {
        **HEADERS,
        "Accept": "application/json, text/plain, */*",
    }

    try:
        async with httpx.AsyncClient(headers=vinted_headers, follow_redirects=True, timeout=15) as client:
            # Krok 1: Pobierz sesjÄ™ z gÅ‚Ã³wnej strony
            session_resp = await client.get("https://www.vinted.pl")
            cookies = session_resp.cookies

            # Krok 2: Pobierz dane z API
            api_url = f"https://www.vinted.pl/api/v2/items/{item_id}"
            resp = await client.get(api_url, cookies=cookies)

            if resp.status_code == 200:
                data = resp.json()
                item = data.get("item", data)

                title = item.get("title", "Brak tytuÅ‚u")
                price_str = item.get("price", {})
                if isinstance(price_str, dict):
                    price = float(price_str.get("amount", "0").replace(",", "."))
                elif isinstance(price_str, str):
                    price = float(price_str.replace(",", "."))
                else:
                    price = float(price_str or 0)

                description = item.get("description", "")
                brand = item.get("brand_dto", {}).get("title", "") if item.get("brand_dto") else ""
                condition = item.get("status", "")
                location = ""
                user_info = item.get("user", {})
                if user_info:
                    city = user_info.get("city", "")
                    country = user_info.get("country_title", "")
                    location = f"{city}, {country}" if city else country
                seller = user_info.get("login", "") if user_info else ""

                # ZdjÄ™cia
                photos = item.get("photos", [])
                images = [p.get("full_size_url", p.get("url", "")) for p in photos[:5]]

                # Dodatkowe info do opisu
                size = item.get("size_title", "")
                color = item.get("color1", {}).get("title", "") if item.get("color1") else ""
                catalogue = item.get("catalog_tree_title", "")

                full_desc = f"{description}"
                if brand:
                    full_desc += f"\nMarka: {brand}"
                if size:
                    full_desc += f"\nRozmiar: {size}"
                if color:
                    full_desc += f"\nKolor: {color}"
                if catalogue:
                    full_desc += f"\nKategoria: {catalogue}"
                if images:
                    full_desc += f"\nZdjÄ™cia: {len(images)} szt."

                return Offer(
                    url=url,
                    title=title,
                    price=price,
                    description=full_desc[:1500],
                    location=location,
                    platform="vinted.pl",
                    seller=seller,
                    condition=condition,
                    images=images,
                    scraped_at=datetime.now().isoformat(),
                )
    except Exception as e:
        logger.warning(f"Vinted API failed: {e}, falling back to HTML scrape")

    # Fallback: HTML scraping (ograniczone dane)
    try:
        async with httpx.AsyncClient(headers=HEADERS, follow_redirects=True, timeout=15) as client:
            resp = await client.get(url)
            resp.raise_for_status()

        soup = BeautifulSoup(resp.text, "html.parser")
        title_tag = soup.find("h1") or soup.find("title")
        title = title_tag.get_text(strip=True) if title_tag else "Brak tytuÅ‚u"

        # Szukaj ceny w meta tagach i JSON-LD
        price = 0.0
        # JSON-LD
        for script in soup.find_all("script", type="application/ld+json"):
            try:
                ld = json.loads(script.string)
                if isinstance(ld, dict) and "offers" in ld:
                    price = float(ld["offers"].get("price", 0))
                    break
            except (json.JSONDecodeError, ValueError):
                pass
        # Meta tag
        if price == 0:
            meta_price = soup.find("meta", {"itemprop": "price"}) or soup.find("meta", {"property": "og:price:amount"})
            if meta_price and meta_price.get("content"):
                try:
                    price = float(meta_price["content"].replace(",", "."))
                except ValueError:
                    pass

        return Offer(
            url=url,
            title=title,
            price=price,
            description=soup.get_text(separator="\n", strip=True)[:1000],
            location="",
            platform="vinted.pl",
            scraped_at=datetime.now().isoformat(),
        )
    except Exception as e:
        logger.error(f"Vinted scrape failed completely: {e}")
        return None


async def scrape_generic(url: str) -> Optional[Offer]:
    """Generyczny scraper â€” prÃ³buje wyciÄ…gnÄ…Ä‡ podstawowe info."""
    async with httpx.AsyncClient(headers=HEADERS, follow_redirects=True, timeout=15) as client:
        resp = await client.get(url)
        resp.raise_for_status()

    soup = BeautifulSoup(resp.text, "html.parser")
    title_tag = soup.find("h1") or soup.find("title")
    title = title_tag.get_text(strip=True) if title_tag else url

    price = 0.0
    price_match = re.search(r'(\d[\d\s]*(?:[.,]\d{2})?)\s*zÅ‚', resp.text)
    if price_match:
        try:
            price = float(price_match.group(1).replace(" ", "").replace(",", "."))
        except ValueError:
            pass

    return Offer(
        url=url,
        title=title,
        price=price,
        description=soup.get_text(separator="\n", strip=True)[:1000],
        location="",
        platform="other",
        scraped_at=datetime.now().isoformat(),
    )


# â”€â”€ SEARCH SCRAPERS (monitoring nowych ofert) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def search_sprzedajemy(keyword: str, max_price: int = MAX_PRICE) -> list[Offer]:
    """Szukaj ofert na Sprzedajemy.pl po sÅ‚owie kluczowym."""
    search_url = f"https://sprzedajemy.pl/szukaj?inp_text={keyword.replace(' ', '+')}"
    offers = []

    try:
        async with httpx.AsyncClient(headers=HEADERS, follow_redirects=True, timeout=15) as client:
            resp = await client.get(search_url)
            resp.raise_for_status()

        soup = BeautifulSoup(resp.text, "html.parser")

        # Sprzedajemy.pl listing items
        items = soup.find_all("a", href=re.compile(r"/.*-nr\d+"))
        seen_urls = set()

        for item in items[:20]:  # max 20 wynikÃ³w
            href = item.get("href", "")
            if not href or href in seen_urls:
                continue
            seen_urls.add(href)

            full_url = f"https://sprzedajemy.pl{href}" if href.startswith("/") else href

            title = item.get_text(strip=True)[:100]
            if not title or len(title) < 3:
                continue

            # PrÃ³buj wyciÄ…gnÄ…Ä‡ cenÄ™ z tekstu
            price = 0.0
            price_match = re.search(r'(\d[\d\s]*)\s*zÅ‚', item.get_text())
            if price_match:
                try:
                    price = float(price_match.group(1).replace(" ", ""))
                except ValueError:
                    pass

            if 0 < price <= max_price or price == 0:
                offers.append(Offer(
                    url=full_url,
                    title=title,
                    price=price,
                    description="",
                    location="",
                    platform="sprzedajemy.pl",
                    scraped_at=datetime.now().isoformat(),
                ))

    except Exception as e:
        logger.error(f"Search error for '{keyword}' on Sprzedajemy: {e}")

    return offers


async def search_gratka(keyword: str, max_price: int = MAX_PRICE) -> list[Offer]:
    """Szukaj ofert na Gratka.pl."""
    search_url = f"https://gratka.pl/szukaj?q={keyword.replace(' ', '+')}"
    offers = []

    try:
        async with httpx.AsyncClient(headers=HEADERS, follow_redirects=True, timeout=15) as client:
            resp = await client.get(search_url)
            resp.raise_for_status()

        soup = BeautifulSoup(resp.text, "html.parser")
        links = soup.find_all("a", href=re.compile(r"gratka\.pl/.*\d"))

        seen = set()
        for link in links[:20]:
            href = link.get("href", "")
            if href in seen or not href:
                continue
            seen.add(href)

            title = link.get_text(strip=True)[:100]
            if len(title) < 3:
                continue

            offers.append(Offer(
                url=href if href.startswith("http") else f"https://gratka.pl{href}",
                title=title,
                price=0,
                description="",
                location="",
                platform="gratka.pl",
                scraped_at=datetime.now().isoformat(),
            ))

    except Exception as e:
        logger.error(f"Search error for '{keyword}' on Gratka: {e}")

    return offers


# â”€â”€ AI ANALYSIS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ANALYSIS_SYSTEM_PROMPT = """JesteÅ› ekspertem od wyceny antykÃ³w, kolekcji i militariÃ³w na polskim rynku wtÃ³rnym.
Twoje zadanie: przeanalizowaÄ‡ ofertÄ™ i daÄ‡ rekomendacjÄ™ kupna/odrzucenia.

KONTEKST UÅ»YTKOWNIKA:
- Profesjonalny reseller z Katowic, specjalizacja: komiksy PRL, porcelana, zegarki vintage, broÅ„ biaÅ‚a, malarstwo, ksiÄ…Å¼ki kolekcjonerskie
- Max cena zakupu: 550 zÅ‚/szt
- Min wymagana marÅ¼a: 200%
- OdbiÃ³r osobisty: max 2h w jednÄ… stronÄ™ od Katowic
- WysyÅ‚ka: OK jeÅ›li jest opcja

TWOJA ANALIZA MUSI ZAWIERAÄ†:
1. **IDENTYFIKACJA** â€” Co to jest? OryginaÅ‚ czy replika? Kluczowe cechy.
2. **RED FLAGS** â€” Co budzi podejrzenia (stan "nowe" na antykach, brak sygnatur, cena typowa dla replik, lakoniczny opis).
3. **WYCENA RYNKOWA** â€” Realistyczny zakres cen na Allegro/domach aukcyjnych dla ORYGINAÅU tego typu.
4. **KALKULACJA** â€” Cena zakupu vs. realistyczna cena sprzedaÅ¼y, marÅ¼a %.
5. **WERDYKT** â€” Jeden z: ğŸŸ¢ KUP (marÅ¼a 200%+, pewny oryginaÅ‚), ğŸŸ¡ NEGOCJUJ (potencjaÅ‚ ale za drogo), ğŸŸ  ZBADAJ (trzeba zobaczyÄ‡ osobiÅ›cie), âŒ OMIÅƒ (replika/za drogo/brak marÅ¼y).

Odpowiadaj zwiÄ™Åºle, maksymalnie 300 sÅ‚Ã³w. Po polsku."""


async def analyze_offer(offer: Offer) -> str:
    """WyÅ›lij ofertÄ™ do Claude API i dostaÅ„ analizÄ™."""
    client = anthropic.AsyncAnthropic(api_key=ANTHROPIC_API_KEY)

    user_message = f"""Przeanalizuj tÄ™ ofertÄ™:

TYTUÅ: {offer.title}
CENA: {offer.price} zÅ‚
STAN: {offer.condition or 'nie podano'}
PLATFORMA: {offer.platform}
LOKALIZACJA: {offer.location}
SPRZEDAWCA: {offer.seller}
OPIS: {offer.description}
URL: {offer.url}
LICZBA ZDJÄ˜Ä†: {len(offer.images)}
ZDJÄ˜CIA: {', '.join(offer.images[:3]) if offer.images else 'brak URL zdjÄ™Ä‡'}"""

    try:
        response = await client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=600,
            system=ANALYSIS_SYSTEM_PROMPT,
            messages=[{"role": "user", "content": user_message}],
        )
        return response.content[0].text
    except Exception as e:
        logger.error(f"AI analysis error: {e}")
        return f"âŒ BÅ‚Ä…d analizy AI: {e}"


def parse_verdict(analysis: str) -> str:
    """WyciÄ…gnij werdykt z analizy AI."""
    if "ğŸŸ¢" in analysis or "KUP" in analysis.upper():
        return "BUY"
    elif "ğŸŸ¡" in analysis or "NEGOCJUJ" in analysis.upper():
        return "NEGOTIATE"
    elif "ğŸŸ " in analysis or "ZBADAJ" in analysis.upper():
        return "INVESTIGATE"
    else:
        return "SKIP"


# â”€â”€ PERSISTENCE (prosty JSON) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

DATA_FILE = "okazje_data.json"
KEYWORDS_FILE = "keywords.json"


def load_seen_urls() -> set:
    """ZaÅ‚aduj URLe ktÃ³re juÅ¼ widzieliÅ›my (Å¼eby nie alertowaÄ‡ dwa razy)."""
    try:
        with open(DATA_FILE, "r") as f:
            data = json.load(f)
            return set(data.get("seen_urls", []))
    except (FileNotFoundError, json.JSONDecodeError):
        return set()


def save_seen_url(url: str):
    """Zapisz URL jako widziany."""
    seen = load_seen_urls()
    seen.add(url)
    # Trzymaj max 5000 URLi
    if len(seen) > 5000:
        seen = set(list(seen)[-3000:])
    with open(DATA_FILE, "w") as f:
        json.dump({"seen_urls": list(seen)}, f)


def load_keywords() -> list[str]:
    try:
        with open(KEYWORDS_FILE, "r") as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return DEFAULT_KEYWORDS.copy()


def save_keywords(keywords: list[str]):
    with open(KEYWORDS_FILE, "w") as f:
        json.dump(keywords, f, ensure_ascii=False, indent=2)


# â”€â”€ SAFE SEND HELPER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def safe_reply(message, text: str):
    """WyÅ›lij wiadomoÅ›Ä‡ â€” najpierw prÃ³buj Markdown, potem plain text."""
    try:
        await message.reply_text(text, parse_mode="Markdown")
    except Exception:
        # JeÅ›li Markdown siÄ™ nie parsuje, wyÅ›lij bez formatowania
        clean = text.replace("**", "").replace("*", "").replace("_", "").replace("`", "")
        try:
            await message.reply_text(clean)
        except Exception as e:
            await message.reply_text(f"BÅ‚Ä…d wysyÅ‚ania: {e}")


# â”€â”€ TELEGRAM HANDLERS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def cmd_start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Komenda /start."""
    chat_id = update.effective_chat.id
    await safe_reply(
        update.message,
        f"ğŸ” OKAZJE BOT â€” TwÃ³j skaner kolekcjonerski\n\n"
        f"ğŸ“‹ Komendy:\n"
        f"â€¢ Wklej link â†’ instant analiza AI\n"
        f"â€¢ /keywords â€” pokaÅ¼/edytuj sÅ‚owa kluczowe\n"
        f"â€¢ /add <sÅ‚owo> â€” dodaj sÅ‚owo kluczowe\n"
        f"â€¢ /remove <sÅ‚owo> â€” usuÅ„ sÅ‚owo kluczowe\n"
        f"â€¢ /scan â€” uruchom skan rÄ™cznie\n"
        f"â€¢ /status â€” status bota\n"
        f"â€¢ /help â€” pomoc\n\n"
        f"ğŸ†” TwÃ³j Chat ID: {chat_id}\n"
        f"(wklej do zmiennej CHAT_ID w Railway)",
    )


async def cmd_help(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await safe_reply(
        update.message,
        "ğŸ” Jak uÅ¼ywaÄ‡:\n\n"
        "1. Analiza linku â€” wklej link z OLX/Vinted/Allegro/Sprzedajemy/Gratka\n"
        "Bot pobierze ofertÄ™, przeanalizuje AI i da Ci werdykt.\n\n"
        "2. MoÅ¼na wkleiÄ‡ wiele linkÃ³w naraz â€” kaÅ¼dy w osobnej linii.\n\n"
        "3. Auto-monitoring â€” bot skanuje Sprzedajemy.pl i Gratka.pl "
        "po Twoich sÅ‚owach kluczowych i wysyÅ‚a alerty o nowych ofertach.\n\n"
        "4. SÅ‚owa kluczowe â€” /keywords, /add, /remove\n\n"
        "Werdykty:\n"
        "ğŸŸ¢ KUP â€” marÅ¼a 200%+, pewny deal\n"
        "ğŸŸ¡ NEGOCJUJ â€” potencjaÅ‚, ale trzeba zbiÄ‡ cenÄ™\n"
        "ğŸŸ  ZBADAJ â€” obejrzyj osobiÅ›cie\n"
        "âŒ OMIÅƒ â€” replika / za drogo / brak marÅ¼y",
    )


async def cmd_keywords(update: Update, context: ContextTypes.DEFAULT_TYPE):
    kw = load_keywords()
    text = "ğŸ”‘ SÅ‚owa kluczowe do monitoringu:\n\n"
    for i, k in enumerate(kw, 1):
        text += f"{i}. {k}\n"
    text += f"\nğŸ“ /add <sÅ‚owo> â€” dodaj\nğŸ“ /remove <numer lub sÅ‚owo> â€” usuÅ„"
    await safe_reply(update.message, text)


async def cmd_add_keyword(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not context.args:
        await update.message.reply_text("UÅ¼ycie: /add <sÅ‚owo kluczowe>")
        return
    keyword = " ".join(context.args)
    kw = load_keywords()
    if keyword in kw:
        await update.message.reply_text(f"'{keyword}' juÅ¼ istnieje na liÅ›cie.")
        return
    kw.append(keyword)
    save_keywords(kw)
    await safe_reply(update.message, f"âœ… Dodano: {keyword}")


async def cmd_remove_keyword(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not context.args:
        await update.message.reply_text("UÅ¼ycie: /remove <numer lub sÅ‚owo kluczowe>")
        return
    arg = " ".join(context.args)
    kw = load_keywords()

    # PrÃ³buj jako numer
    try:
        idx = int(arg) - 1
        if 0 <= idx < len(kw):
            removed = kw.pop(idx)
            save_keywords(kw)
            await safe_reply(update.message, f"âœ… UsuniÄ™to: {removed}")
            return
    except ValueError:
        pass

    # PrÃ³buj jako tekst
    if arg in kw:
        kw.remove(arg)
        save_keywords(kw)
        await safe_reply(update.message, f"âœ… UsuniÄ™to: {arg}")
    else:
        await update.message.reply_text(f"Nie znaleziono '{arg}' na liÅ›cie.")


async def cmd_status(update: Update, context: ContextTypes.DEFAULT_TYPE):
    kw = load_keywords()
    seen = load_seen_urls()
    await update.message.reply_text(
        f"ğŸ“Š **Status bota:**\n"
        f"â€¢ SÅ‚owa kluczowe: {len(kw)}\n"
        f"â€¢ Widziane oferty: {len(seen)}\n"
        f"â€¢ InterwaÅ‚ skanowania: {SCAN_INTERVAL_MINUTES} min\n"
        f"â€¢ Max cena: {MAX_PRICE} zÅ‚\n"
        f"â€¢ Min marÅ¼a: {MIN_MARGIN_PERCENT}%\n"
        f"â€¢ Platformy monitorowane: Sprzedajemy.pl, Gratka.pl\n"
        f"â€¢ Platformy rÄ™czne: OLX, Vinted, Allegro, eBay",

    )


async def cmd_scan(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """RÄ™czne uruchomienie skanu."""
    await update.message.reply_text("ğŸ”„ Uruchamiam skan... to moÅ¼e chwilÄ™ potrwaÄ‡.")
    found = await run_scan(context.bot, str(update.effective_chat.id))
    if found == 0:
        await update.message.reply_text("Brak nowych ofert speÅ‚niajÄ…cych kryteria.")


async def handle_links(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Handler dla wklejonych linkÃ³w â€” gÅ‚Ã³wna funkcja bota."""
    text = update.message.text or ""

    # WyciÄ…gnij wszystkie URLe
    urls = re.findall(r'https?://\S+', text)

    if not urls:
        await update.message.reply_text(
            "Nie znalazÅ‚em linku. Wklej link do oferty z OLX/Vinted/Allegro/Sprzedajemy/Gratka."
        )
        return

    for url in urls:
        # Clean URL
        url = url.rstrip(".,;:!?)")

        await update.message.reply_text(f"ğŸ” Pobieram: {url[:60]}...")

        offer = await scrape_url(url)

        if not offer:
            await update.message.reply_text(
                f"âŒ Nie udaÅ‚o siÄ™ pobraÄ‡ oferty z:\n{url}\n\n"
                f"MoÅ¼esz wkleiÄ‡ opis rÄ™cznie â€” przeanalizujÄ™ go."
            )
            continue

        # Podsumowanie scrape'a
        img_info = f"ğŸ“· ZdjÄ™cia: {len(offer.images)}" if offer.images else "ğŸ“· Brak zdjÄ™Ä‡"
        await safe_reply(
            update.message,
            f"ğŸ“¦ {offer.title}\n"
            f"ğŸ’° Cena: {offer.price} zÅ‚\n"
            f"ğŸ“ {offer.location or 'brak lokalizacji'}\n"
            f"ğŸ“„ Stan: {offer.condition or 'nie podano'}\n"
            f"{img_info}\n\n"
            f"ğŸ¤– AnalizujÄ™ z AI...",
        )

        # AI analysis
        analysis = await analyze_offer(offer)
        offer.analysis = analysis
        offer.verdict = parse_verdict(analysis)

        # Zapisz jako widziane
        save_seen_url(url)

        # WyÅ›lij analizÄ™
        verdict_emoji = {
            "BUY": "ğŸŸ¢", "NEGOTIATE": "ğŸŸ¡",
            "INVESTIGATE": "ğŸŸ ", "SKIP": "âŒ"
        }
        emoji = verdict_emoji.get(offer.verdict, "â“")

        await safe_reply(
            update.message,
            f"{emoji} ANALIZA: {offer.title[:50]}\n\n{analysis}",
        )


async def handle_text(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Handler dla tekstu bez linkÃ³w â€” traktuj jako rÄ™czny opis oferty."""
    text = update.message.text or ""

    if len(text) < 20:
        await update.message.reply_text(
            "Wklej link do oferty lub opisz przedmiot (min. 20 znakÃ³w) do analizy."
        )
        return

    # Traktuj jako rÄ™czny opis
    offer = Offer(
        url="rÄ™czny opis",
        title=text[:50],
        price=0,
        description=text,
        location="",
        platform="manual",
        scraped_at=datetime.now().isoformat(),
    )

    await update.message.reply_text("ğŸ¤– AnalizujÄ™ opis z AI...")
    analysis = await analyze_offer(offer)

    await safe_reply(
        update.message,
        f"ğŸ“‹ ANALIZA OPISU:\n\n{analysis}",
    )


# â”€â”€ AUTO-SCAN JOB â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def run_scan(bot: Bot, chat_id: str) -> int:
    """Skanuj Sprzedajemy i Gratka po sÅ‚owach kluczowych."""
    if not chat_id:
        logger.warning("No CHAT_ID set, skipping auto-scan alerts.")
        return 0

    keywords = load_keywords()
    seen = load_seen_urls()
    new_offers = []

    for keyword in keywords:
        # Sprzedajemy
        try:
            offers = await search_sprzedajemy(keyword)
            for o in offers:
                if o.url not in seen and o.price <= MAX_PRICE:
                    new_offers.append(o)
                    save_seen_url(o.url)
        except Exception as e:
            logger.error(f"Scan error Sprzedajemy '{keyword}': {e}")

        # Gratka
        try:
            offers = await search_gratka(keyword)
            for o in offers:
                if o.url not in seen:
                    new_offers.append(o)
                    save_seen_url(o.url)
        except Exception as e:
            logger.error(f"Scan error Gratka '{keyword}': {e}")

        # Rate limiting â€” nie bombarduj serwerÃ³w
        await asyncio.sleep(2)

    if not new_offers:
        logger.info(f"Scan complete: 0 new offers.")
        return 0

    # Ogranicz do 10 najciekawszych (po cenie â€” niÅ¼sze = ciekawsze)
    new_offers.sort(key=lambda o: o.price if o.price > 0 else 9999)
    top_offers = new_offers[:10]

    # WyÅ›lij alert
    alert_text = f"ğŸ”” **NOWE OFERTY** ({len(new_offers)} znalezionych)\n\n"
    for i, o in enumerate(top_offers, 1):
        alert_text += (
            f"{i}. {o.title[:50]}\n"
            f"ğŸ’° {o.price} zÅ‚ | ğŸ“ {o.platform}\n"
            f"ğŸ”— {o.url}\n\n"
        )

    if len(new_offers) > 10:
        alert_text += f"...i {len(new_offers) - 10} wiÄ™cej\n"

    alert_text += "\nğŸ’¡ Wklej interesujÄ…cy link, Å¼eby dostaÄ‡ peÅ‚nÄ… analizÄ™ AI."

    try:
        await bot.send_message(chat_id=chat_id, text=alert_text)
    except Exception as e:
        logger.error(f"Failed to send alert: {e}")

    logger.info(f"Scan complete: {len(new_offers)} new offers, {len(top_offers)} sent.")
    return len(new_offers)


async def scheduled_scan(context: ContextTypes.DEFAULT_TYPE):
    """Job do automatycznego skanowania."""
    if CHAT_ID:
        await run_scan(context.bot, CHAT_ID)


# â”€â”€ MAIN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    """Uruchom bota."""
    logger.info("Starting Okazje Bot...")

    app = Application.builder().token(TELEGRAM_TOKEN).build()

    # Handlers
    app.add_handler(CommandHandler("start", cmd_start))
    app.add_handler(CommandHandler("help", cmd_help))
    app.add_handler(CommandHandler("keywords", cmd_keywords))
    app.add_handler(CommandHandler("add", cmd_add_keyword))
    app.add_handler(CommandHandler("remove", cmd_remove_keyword))
    app.add_handler(CommandHandler("status", cmd_status))
    app.add_handler(CommandHandler("scan", cmd_scan))

    # Link handler (priority)
    app.add_handler(MessageHandler(filters.TEXT & filters.Regex(r'https?://'), handle_links))

    # Text handler (fallback)
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_text))

    # Scheduled scan
    if CHAT_ID:
        job_queue = app.job_queue
        job_queue.run_repeating(
            scheduled_scan,
            interval=SCAN_INTERVAL_MINUTES * 60,
            first=60,  # Pierwszy skan po 1 minucie
        )
        logger.info(f"Auto-scan enabled every {SCAN_INTERVAL_MINUTES} min for chat {CHAT_ID}")
    else:
        logger.warning("CHAT_ID not set â€” auto-scan alerts disabled. Use /start to get your ID.")

    # Run
    app.run_polling(drop_pending_updates=True)


if __name__ == "__main__":
    main()
